%!TEX root = ../TTK4550-MHT.tex
\section{Implementation}
\label{sec:implementation}
The aim of this section is to elaborate the decisions that were made during the implementation of the algorithm outlined in section \ref{sec:algorithm} and \ref{sec:ilp}.

\subsection{Programming language}
This work in done as a part of a larger project (\gls{autosea}) where all code is developed in \gls{python} or C++. \Gls{python} is known for its large pool of modules, easy portability and quick programming, which makes it a great choice for research and development of algorithms. It is far from the fastest out-of-the-box programming language available, there are however side versions of python like \gls{cython} that allows for optimisation and compilation of \gls{python} to C code. If \gls{gpu} acceleration should be possible and desirable, \gls{python} also have modules that allows for very easy and powerful usage of \gls{cuda} cores from Nvidia products.

%To maximize the capability with other code in this project, all implementation was done in \gls{python} 3. Further was data storage and calculations done as much as possible in modules like NumPy and SciPy, which both have excellent runtime for many common operations. 

\subsection{Code structure}
The code is structured in an object oriented way with two primary classes, a \emph{Tracker} and a \emph{Target} class. The Tracker is the class the user initializes and configures with parameters that are not target specific, like \gls{Pd}, $\lambda_\nu$, $\lambda_\phi$, $\eta^2$, $N$ and state space model. The Tracker is also the root for the track forest, meaning that all tracks are trees of Target objects originating from a single Tracker object.

When a new track is initialised, an \emph{initiateTarget} procedure is called on the tracker, and a new root for a \gls{track hypothesis tree} is created in the form of a Target object. As new \glspl{scan} are received, the trackersÂ´ \emph{addMeasurementList} method is called with the measurements, where upon all \glspl{target} are predicted, gated and created, scored, pruned, selected and pruned, before the result is return to the user, see Figure \ref{fig:algorithm_flow}.

Of the aforementioned steps, the prediction, gating and creation of new \glspl{track hypothesis} will be referred to as the \emph{tree growing} task. The growing of the tree is in the single-core case implemented as a recursive function that traverse the track forest and gates and creates new nodes whenever it encounter a leaf node as described in Algorithm \ref{alg:processNewMeasurement}.
\begin{algorithm}[H]
\caption{Tree growing}
\label{alg:processNewMeasurement}
\begin{algorithmic}[1]
\Procedure{Target::processNewMeasurements}{}
\If {$\textit{self.childs} = \textit{None}$}
	\State $\textit{self.predictMeasurement}$
	\State $\textit{self.gateAndCreateNewNodes}$
\Else
	\For $ \textit{ node \textbf{in} self.children} $
		\State $\textit{node.processNewMeasurements}$
	\EndFor
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}
In addition to expanding the tree structure, the tree growing procedure updates a \emph{set}\footnote{Python: Unordered collections of unique elements} stored for each target, containing the measurements in the target tree. This set contains no new information compared to the tree structure, but increases the speed of the clustering later on since a set is hashed and can be accessed in constant time. The multi-core implementation of the tree growing is done slightly different, since the worker processes can not access the tree structure directly in Python. This is because Python pickle arguments that are sent to other processes, which is a procedure that converts the arguments to a byte stream. The effect  of this pickling is that arguments are effectively passed as copies instead of reference. The workaround for this is to run a procedure on each leaf node where a copy of that node is sent, and a processed node with added measurements are returned. Through this process, the copying of the (large) tree structure  is avoided. These steps requires a complete list of all leaf nodes, which can be obtained fast through traversing the forest.

The clustering of the targets is done through building an adjacency matrix where all targets and nodes (measurements) in the track forest are represented. Targets and nodes are connected with an edges where there is a usage of that measurement. The adjacency matrix is then used in a connected components algorithm which returns labels on all the nodes according to their clustering.

After the clustering, each the algorithm iterates the clusters and solves the optimization problem for clusters with more than one targets, and searches for the best hypothesis through traversing on clusters with one target.

\subsection{Performance considerations}
Of the aforementioned steps, the growing is the most time consuming. For example, a scan which takes 1758 ms in total to process, where 1496 ms are used to grow the \gls{track hypothesis tree} when running on a single \gls{cpu} core. Since the search after leaf nodes is little time consuming, there is a possibility for running all the leaf nodes in the track forest in parallel on a \gls{gpu}. A \gls{gpu} is build with a high number of cores, typically in the range from 200 to 2000. In comparison with \gls{cpu} cores which is designed to execute different logical operations on each core at the same time, \gls{gpu} cores are designed to execute the same logical operation on all its cores at the same time. This is very favourable for the process of tree growing.

According to Nvidia \footnote{https://developer.nvidia.com/how-to-cuda-python}, it is possible to get performance increase up to 20x-2000x, when running pure \Gls{python} code on \Glspl{gpu}. This could potentially reduce the grow time from about 1500 ms to 75-1 ms.